{% set name = "evaluate" %}
{% set version = "0.4.6" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  - url: https://pypi.org/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz
    sha256: e07036ca12b3c24331f83ab787f21cc2dbf3631813a1631e63e40897c69a3f21
  - url: https://github.com/huggingface/evaluate/archive/refs/tags/v0.4.3.tar.gz
    sha256: af193874d5fdd3c4321e1b740e3f67232fac950d357dfd9034337e8c17a5d09e
    folder: gh_src

build:
  number: 0
  entry_points:
    - evaluate-cli=evaluate.commands.evaluate_cli:main
  script: {{ PYTHON }} -m pip install . -vv --no-deps --no-build-isolation
  skip: true  # [py<38]

requirements:
  host:
    - python
    - pip
    - setuptools
    - wheel
  run:
    - python
    - datasets >=2.0.0
    - numpy >=1.17
    - dill
    - pandas
    - requests >=2.19.0
    - tqdm >=4.62.1
    - python-xxhash
    - multiprocess
    - importlib-metadata  # [py<38]
    # fsspec[http]
    - fsspec >=2021.05.0
    - aiohttp
    - huggingface_hub >=0.7.0
    - packaging
    # Needed for evaluate-cli command.
    - cookiecutter
  run_constrained:
    # Extra: evaluator
    - scipy >=1.7.1
    # Extra: template
    - gradio >=3.0.0
    # Extra: tensorflow
    - tensorflow-base >=2.2.0,!=2.6.0,!=2.6.1

# Disabled by upstream: https://github.com/huggingface/evaluate/issues/632
{% set ignore_tests = " --ignore=gh_src/tests/test_trainer_evaluator_parity.py" %}
# E   absl.testing.parameterized.NoTestsError: parameterized test decorators did not generate any tests. 
#     Make sure you specify non-empty parameters, and do not reuse generators more than once.
{% set ignore_tests = ignore_tests + " --ignore=gh_src/tests/test_metric_common.py" %}
# E   Module not found - "rouge_score"
{% set tests_to_skip = "test_device_placement" %}
{% set tests_to_skip = tests_to_skip + " or test_overwrite_default_metric" %}
{% set tests_to_skip = tests_to_skip + " or test_summarization" %}
# E   Module not found - "seqeval"
{% set ignore_tests = ignore_tests + " --ignore=gh_src/tests/test_evaluator.py" %}

test:
  source_files:
    - gh_src/tests
  imports:
    - evaluate
  commands:
    - pip check
    - evaluate-cli --help
    # run only unit tests like in upstream CI.
    - pytest -n 2 --dist loadfile -sv gh_src/tests {{ ignore_tests }} -k "not ({{ tests_to_skip }})"
  requires:
    - pip
    - pytest
    - pytest-xdist
    - matplotlib-base
    - absl-py
    - pytorch
    - pillow
    - transformers
    - scikit-learn
    - scipy >=1.7.1
    - jiwer
    - nltk
    - git  # [linux]

about:
  home: https://github.com/huggingface/evaluate
  summary: HuggingFace community-driven open-source library of evaluation
  description: |
    Evaluate is a library that makes evaluating and comparing models and reporting their performance easier and more standardized.

    It currently contains:

      - implementations of dozens of popular metrics: the existing metrics cover a variety of tasks spanning from NLP to Computer Vision, and include dataset-specific metrics for datasets. With a simple command like `accuracy = load("accuracy")`, get any of these metrics ready to use for evaluating a ML model in any framework (Numpy/Pandas/PyTorch/TensorFlow/JAX).
      - comparisons and measurements: comparisons are used to measure the difference between models and measurements are tools to evaluate datasets.
      - an easy way of adding new evaluation modules to the ðŸ¤— Hub: you can create new evaluation modules and push them to a dedicated Space in the ðŸ¤— Hub with evaluate-cli create [metric name], which allows you to see easily compare different metrics and their outputs for the same sets of references and predictions.
  license: Apache-2.0
  license_file: LICENSE
  license_family: Apache
  doc_url: https://huggingface.co/docs/evaluate/index
  dev_url: https://github.com/huggingface/evaluate

extra:
  recipe-maintainers:
    - wietsedv