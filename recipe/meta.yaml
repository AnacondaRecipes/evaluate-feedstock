{% set name = "evaluate" %}
{% set version = "0.3.0" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/evaluate-{{ version }}.tar.gz
  sha256: 4c3652306064bcf7bf87d548f244d0404d6b7a67bae76b6dc2f9f55402f26052

build:
  # Skip s390x until datasets >=2.0.0 is available
  skip: true  # [py<37 or s390x]
  entry_points:
    - evaluate-cli=evaluate.commands.evaluate_cli:main
  script: {{ PYTHON }} -m pip install . -vv
  number: 0

requirements:
  host:
    - python
    - pip
    - setuptools
    - wheel
  run:
    - python
    - datasets >=2.0.0
    - numpy >=1.17
    - dill
    - pandas
    - requests >=2.19.0
    - tqdm >=4.62.1
    - python-xxhash
    - multiprocess
    - importlib-metadata
    - fsspec >=2021.05.0
    - huggingface_hub >=0.7.0
    - packaging
    - responses <0.19
    - cookiecutter # to populate metric template

test:
  imports:
    - evaluate
  commands:
    - pip check
    - evaluate-cli --help
  requires:
    - pip

about:
  home: https://github.com/huggingface/evaluate
  summary: HuggingFace community-driven open-source library of evaluation
  description: |
    Evaluate is a library that makes evaluating and comparing models and reporting their performance easier and more standardized.

    It currently contains:

      - implementations of dozens of popular metrics: the existing metrics cover a variety of tasks spanning from NLP to Computer Vision, and include dataset-specific metrics for datasets. With a simple command like `accuracy = load("accuracy")`, get any of these metrics ready to use for evaluating a ML model in any framework (Numpy/Pandas/PyTorch/TensorFlow/JAX).
      - comparisons and measurements: comparisons are used to measure the difference between models and measurements are tools to evaluate datasets.
      - an easy way of adding new evaluation modules to the ğŸ¤— Hub: you can create new evaluation modules and push them to a dedicated Space in the ğŸ¤— Hub with evaluate-cli create [metric name], which allows you to see easily compare different metrics and their outputs for the same sets of references and predictions.
  license: Apache-2.0
  license_file: LICENSE
  license_family: Apache
  license_url: https://github.com/huggingface/evaluate/blob/main/LICENSE
  doc_url: https://huggingface.co/docs/evaluate/index
  doc_source_url: https://github.com/huggingface/evaluate/tree/main/docs
  dev_url: https://github.com/huggingface/evaluate

extra:
  recipe-maintainers:
    - wietsedv
